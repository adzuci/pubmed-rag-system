{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# PubMed Processing + Quick Relevance Check\n",
    "\n",
    "This notebook was created with the help of Cursor to review and process the data being fetched. It reads the fetched `.txt` records from `data/pubmed_fetch/`, normalizes them into a simple structure, and runs a lightweight relevance check to see if the search filter is producing useful articles.\n",
    "\n",
    "It is intentionally simple and fast to run locally before investing time in chunking/embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "FETCH_DIR = \"data/pubmed_fetch\"\n",
    "paths = sorted(glob.glob(os.path.join(FETCH_DIR, \"*.txt\")))\n",
    "\n",
    "if not paths:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No records found in {FETCH_DIR}. Run the search notebook first.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def parse_record(text):\n",
    "    record = {\n",
    "        \"pmid\": None,\n",
    "        \"title\": \"\",\n",
    "        \"authors\": \"\",\n",
    "        \"journal\": \"\",\n",
    "        \"date\": \"\",\n",
    "        \"abstract\": \"\",\n",
    "    }\n",
    "    abstract_lines = []\n",
    "    in_abstract = False\n",
    "\n",
    "    for line in text.splitlines():\n",
    "        if line.startswith(\"PMID: \"):\n",
    "            record[\"pmid\"] = line.replace(\"PMID: \", \"\").strip()\n",
    "            continue\n",
    "        if line.startswith(\"Title: \"):\n",
    "            record[\"title\"] = line.replace(\"Title: \", \"\").strip()\n",
    "            continue\n",
    "        if line.startswith(\"Authors: \"):\n",
    "            record[\"authors\"] = line.replace(\"Authors: \", \"\").strip()\n",
    "            continue\n",
    "        if line.startswith(\"Journal: \"):\n",
    "            record[\"journal\"] = line.replace(\"Journal: \", \"\").strip()\n",
    "            continue\n",
    "        if line.startswith(\"Date: \"):\n",
    "            record[\"date\"] = line.replace(\"Date: \", \"\").strip()\n",
    "            continue\n",
    "        if line.startswith(\"Abstract:\"):\n",
    "            in_abstract = True\n",
    "            abstract_lines.append(line.replace(\"Abstract:\", \"\").lstrip())\n",
    "            continue\n",
    "        if in_abstract:\n",
    "            abstract_lines.append(line)\n",
    "\n",
    "    record[\"abstract\"] = \"\\n\".join([line for line in abstract_lines if line]).strip()\n",
    "    return record\n",
    "\n",
    "\n",
    "records = []\n",
    "for path in paths:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as handle:\n",
    "        records.append(parse_record(handle.read()))\n",
    "\n",
    "len(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def normalize(text):\n",
    "    return re.sub(r\"\\s+\", \" \", text or \"\").strip().lower()\n",
    "\n",
    "\n",
    "signal_terms = [\n",
    "    \"caregiver\",\n",
    "    \"caregiving\",\n",
    "    \"decision support\",\n",
    "    \"clinical decision support\",\n",
    "    \"cdss\",\n",
    "    \"dementia\",\n",
    "    \"alzheimer\",\n",
    "    \"mild cognitive impairment\",\n",
    "]\n",
    "\n",
    "\n",
    "def has_signal(rec):\n",
    "    haystack = normalize(f\"{rec.get('title', '')} {rec.get('abstract', '')}\")\n",
    "    return any(term in haystack for term in signal_terms)\n",
    "\n",
    "\n",
    "with_abstract = sum(1 for rec in records if rec.get(\"abstract\"))\n",
    "signal_hits = [rec for rec in records if has_signal(rec)]\n",
    "\n",
    "avg_abstract_len = (\n",
    "    sum(len(rec.get(\"abstract\", \"\")) for rec in records) / max(len(records), 1)\n",
    ")\n",
    "\n",
    "journal_counts = Counter(rec.get(\"journal\", \"\").strip() for rec in records)\n",
    "\n",
    "summary = {\n",
    "    \"total_records\": len(records),\n",
    "    \"with_abstract\": with_abstract,\n",
    "    \"with_abstract_pct\": round(with_abstract / max(len(records), 1) * 100, 1),\n",
    "    \"signal_match_pct\": round(len(signal_hits) / max(len(records), 1) * 100, 1),\n",
    "    \"avg_abstract_len_chars\": int(avg_abstract_len),\n",
    "    \"top_journals\": journal_counts.most_common(5),\n",
    "}\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick spot-check: titles that did NOT match the simple signal terms.\n",
    "no_signal_titles = [rec.get(\"title\") for rec in records if not has_signal(rec)]\n",
    "no_signal_titles[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def normalize_whitespace(text):\n",
    "    return re.sub(r\"\\s+\", \" \", text or \"\").strip()\n",
    "\n",
    "\n",
    "def normalize_date(value):\n",
    "    # Best-effort normalization to YYYY-MM-DD; fallback to original\n",
    "    value = (value or \"\").strip()\n",
    "    if not value:\n",
    "        return \"\"\n",
    "    # Examples observed: \"2026 Jan 7\", \"2025 Dec\", \"2025\"\n",
    "    try:\n",
    "        return datetime.strptime(value, \"%Y %b %d\").strftime(\"%Y-%m-%d\")\n",
    "    except ValueError:\n",
    "        pass\n",
    "    try:\n",
    "        return datetime.strptime(value, \"%Y %b\").strftime(\"%Y-%m-01\")\n",
    "    except ValueError:\n",
    "        pass\n",
    "    if re.fullmatch(r\"\\d{4}\", value):\n",
    "        return f\"{value}-01-01\"\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"data\"\n",
    "RUN_DATE = datetime.utcnow().strftime(\"%Y%m%d\")\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_DIR, f\"pubmed_records_{RUN_DATE}.jsonl\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "record_docs = []\n",
    "for rec in records:\n",
    "    title = normalize_whitespace(rec.get(\"title\", \"\"))\n",
    "    abstract = normalize_whitespace(rec.get(\"abstract\", \"\"))\n",
    "    full_text = \"\\n\".join([t for t in [title, abstract] if t])\n",
    "    record_docs.append(\n",
    "        {\n",
    "            \"id\": rec.get(\"pmid\"),\n",
    "            \"text\": full_text,\n",
    "            \"metadata\": {\n",
    "                \"pmid\": rec.get(\"pmid\"),\n",
    "                \"title\": title,\n",
    "                \"journal\": rec.get(\"journal\"),\n",
    "                \"authors\": rec.get(\"authors\"),\n",
    "                \"date\": normalize_date(rec.get(\"date\")),\n",
    "                \"source\": \"pubmed_fetch\",\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as handle:\n",
    "    for doc in record_docs:\n",
    "        handle.write(json.dumps(doc, ensure_ascii=True) + \"\\n\")\n",
    "\n",
    "OUTPUT_PATH, len(record_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3 processed prefix\n",
    "S3_BUCKET = os.getenv(\"S3_BUCKET\", \"\")\n",
    "S3_PREFIX = os.getenv(\"S3_PREFIX\", \"processed/\")\n",
    "\n",
    "if not S3_BUCKET:\n",
    "    raise ValueError(\"Set S3_BUCKET in your environment or .env before upload.\")\n",
    "\n",
    "s3_uri = f\"s3://{S3_BUCKET}/{S3_PREFIX}\"\n",
    "\n",
    "import subprocess\n",
    "\n",
    "subprocess.run([\n",
    "    \"aws\",\n",
    "    \"s3\",\n",
    "    \"cp\",\n",
    "    OUTPUT_PATH,\n",
    "    s3_uri,\n",
    "], check=True)\n",
    "\n",
    "s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kick off Bedrock KB ingestion\n",
    "KB_ID = os.getenv(\"BEDROCK_KB_ID\", \"\")\n",
    "DATA_SOURCE_ID = os.getenv(\"BEDROCK_KB_DATA_SOURCE_ID\", \"\")\n",
    "\n",
    "if not KB_ID or not DATA_SOURCE_ID:\n",
    "    raise ValueError(\"Set BEDROCK_KB_ID and BEDROCK_KB_DATA_SOURCE_ID in your env or .env\")\n",
    "\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\n",
    "        \"aws\",\n",
    "        \"bedrock-agent\",\n",
    "        \"start-ingestion-job\",\n",
    "        \"--knowledge-base-id\",\n",
    "        KB_ID,\n",
    "        \"--data-source-id\",\n",
    "        DATA_SOURCE_ID,\n",
    "    ],\n",
    "    check=True,\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    ")\n",
    "\n",
    "payload = json.loads(result.stdout)\n",
    "job_id = payload[\"ingestionJob\"][\"ingestionJobId\"]\n",
    "{\n",
    "    \"knowledgeBaseId\": KB_ID,\n",
    "    \"dataSourceId\": DATA_SOURCE_ID,\n",
    "    \"ingestionJobId\": job_id,\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pubmed-rag-system",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
