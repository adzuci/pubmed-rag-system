{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0",
      "metadata": {},
      "source": [
        "# PubMed Processing + Quick Relevance Check\n",
        "\n",
        "## Notebook outline\n",
        "\n",
        "| Cell | Purpose |\n",
        "|------|---------|\n",
        "| 1 | Load .txt paths from data/pubmed_fetch/, parse each into a record dict, build records list |\n",
        "| 2 | Define signal terms and has_signal; compute summary stats (abstracts, signal match %, journals) |\n",
        "| 3 | Spot-check: titles that did not match signal terms (first 10) |\n",
        "| 4 | Define normalize_whitespace and normalize_date for export |\n",
        "| 5 | Build record_docs (id, text, metadata), write JSONL to data/pubmed_records_YYYYMMDD.jsonl |\n",
        "| 6 | Optional: upload JSONL to S3 processed/ prefix (if S3_BUCKET set) |\n",
        "| 7 | Start Bedrock KB ingestion job (requires BEDROCK_KB_ID, BEDROCK_KB_DATA_SOURCE_ID) |\n",
        "\n",
        "---\n",
        "\n",
        "**Prerequisites.** Run the search notebook first so `data/pubmed_fetch/` exists.\n",
        "\n",
        "This notebook reads the fetched `.txt` records from `data/pubmed_fetch/`, normalizes them into a simple structure, and runs a lightweight relevance check to see if the search filter is producing useful articles. It is intentionally simple and fast to run locally before investing time in chunking/embedding.\n",
        "\n",
        "**Env (optional).** Cells 6–7 use: `S3_BUCKET`, `S3_PREFIX` (cell 6); `BEDROCK_KB_ID`, `BEDROCK_KB_DATA_SOURCE_ID` (cell 7). Set in `.env` or your shell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "1",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cell 1: Load .txt paths from data/pubmed_fetch/, parse each into a record dict, build records list.\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import Any\n",
        "\n",
        "\n",
        "def load_env(reload: bool = False) -> str | None:\n",
        "    \"\"\"Load .env if present; returns the resolved path when found.\"\"\"\n",
        "    try:\n",
        "        from dotenv import find_dotenv, load_dotenv\n",
        "\n",
        "        dotenv_path = find_dotenv(usecwd=True) or str(Path(\".env\").resolve())\n",
        "        load_dotenv(dotenv_path=dotenv_path, override=reload)\n",
        "        return dotenv_path\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "DOTENV_PATH = load_env()\n",
        "\n",
        "FETCH_DIR: str = \"data/pubmed_fetch\"\n",
        "paths: list[str] = sorted(glob.glob(os.path.join(FETCH_DIR, \"*.txt\")))\n",
        "\n",
        "if not paths:\n",
        "    raise FileNotFoundError(\n",
        "        f\"No records found in {FETCH_DIR}. Run the search notebook first.\"\n",
        "    )\n",
        "\n",
        "\n",
        "def parse_record(text: str) -> dict[str, Any]:\n",
        "    \"\"\"Parse our .txt fetch format (MEDLINE-derived; see search notebook) into a simple dict (pmid, title, authors, journal, date, abstract).\"\"\"\n",
        "    record: dict[str, Any] = {\n",
        "        \"pmid\": None,\n",
        "        \"title\": \"\",\n",
        "        \"authors\": \"\",\n",
        "        \"journal\": \"\",\n",
        "        \"date\": \"\",\n",
        "        \"abstract\": \"\",\n",
        "    }\n",
        "    abstract_lines = []\n",
        "    in_abstract = False\n",
        "\n",
        "    for line in text.splitlines():\n",
        "        if line.startswith(\"PMID: \"):\n",
        "            record[\"pmid\"] = line.replace(\"PMID: \", \"\").strip()\n",
        "            continue\n",
        "        if line.startswith(\"Title: \"):\n",
        "            record[\"title\"] = line.replace(\"Title: \", \"\").strip()\n",
        "            continue\n",
        "        if line.startswith(\"Authors: \"):\n",
        "            record[\"authors\"] = line.replace(\"Authors: \", \"\").strip()\n",
        "            continue\n",
        "        if line.startswith(\"Journal: \"):\n",
        "            record[\"journal\"] = line.replace(\"Journal: \", \"\").strip()\n",
        "            continue\n",
        "        if line.startswith(\"Date: \"):\n",
        "            record[\"date\"] = line.replace(\"Date: \", \"\").strip()\n",
        "            continue\n",
        "        if line.startswith(\"Abstract:\"):\n",
        "            in_abstract = True\n",
        "            abstract_lines.append(line.replace(\"Abstract:\", \"\").lstrip())\n",
        "            continue\n",
        "        if in_abstract:\n",
        "            abstract_lines.append(line)\n",
        "\n",
        "    record[\"abstract\"] = \"\\n\".join([line for line in abstract_lines if line]).strip()\n",
        "    return record\n",
        "\n",
        "\n",
        "records: list[dict[str, Any]] = []\n",
        "for path in paths:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as handle:\n",
        "        records.append(parse_record(handle.read()))\n",
        "\n",
        "len(records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "2",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'total_records': 500,\n",
              " 'with_abstract': 500,\n",
              " 'with_abstract_pct': 100.0,\n",
              " 'signal_match_pct': 99.8,\n",
              " 'avg_abstract_len_chars': 1866,\n",
              " 'top_journals': [(\"Alzheimer's & dementia : the journal of the Alzheimer's Association\",\n",
              "   37),\n",
              "  (\"Journal of Alzheimer's disease : JAD\", 21),\n",
              "  ('The Gerontologist', 21),\n",
              "  ('BMC geriatrics', 19),\n",
              "  ('BMJ open', 16)]}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cell 2: Define signal terms and has_signal; compute summary stats (abstracts, signal match %, journals).\n",
        "from collections import Counter\n",
        "from typing import Any\n",
        "\n",
        "\n",
        "def normalize(text: str | None) -> str:\n",
        "    \"\"\"Collapse whitespace and lowercase for simple term matching.\"\"\"\n",
        "    return re.sub(r\"\\s+\", \" \", text or \"\").strip().lower()\n",
        "\n",
        "\n",
        "signal_terms: list[str] = [\n",
        "    \"caregiver\",\n",
        "    \"caregiving\",\n",
        "    \"decision support\",\n",
        "    \"clinical decision support\",\n",
        "    \"cdss\",\n",
        "    \"dementia\",\n",
        "    \"alzheimer\",\n",
        "    \"mild cognitive impairment\",\n",
        "]\n",
        "\n",
        "\n",
        "def has_signal(rec: dict[str, Any]) -> bool:\n",
        "    \"\"\"True if the record's title or abstract contains any of our signal terms (caregiver, dementia, etc.).\"\"\"\n",
        "    haystack: str = normalize(f\"{rec.get('title', '')} {rec.get('abstract', '')}\")\n",
        "    return any(term in haystack for term in signal_terms)\n",
        "\n",
        "\n",
        "with_abstract: int = sum(1 for rec in records if rec.get(\"abstract\"))\n",
        "signal_hits: list[dict[str, Any]] = [rec for rec in records if has_signal(rec)]\n",
        "\n",
        "avg_abstract_len: float = sum(len(rec.get(\"abstract\", \"\")) for rec in records) / max(\n",
        "    len(records), 1\n",
        ")\n",
        "\n",
        "journal_counts: Counter[str] = Counter(\n",
        "    rec.get(\"journal\", \"\").strip() for rec in records\n",
        ")\n",
        "\n",
        "summary: dict[str, Any] = {\n",
        "    \"total_records\": len(records),\n",
        "    \"with_abstract\": with_abstract,\n",
        "    \"with_abstract_pct\": round(with_abstract / max(len(records), 1) * 100, 1),\n",
        "    \"signal_match_pct\": round(len(signal_hits) / max(len(records), 1) * 100, 1),\n",
        "    \"avg_abstract_len_chars\": int(avg_abstract_len),\n",
        "    \"top_journals\": journal_counts.most_common(5),\n",
        "}\n",
        "\n",
        "summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"Emotion recognition in people with Huntington's disease: A comprehensive systematic review.\"]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cell 3: Spot-check — show titles that did not match signal terms (first 10).\n",
        "no_signal_titles: list[str | None] = [\n",
        "    rec.get(\"title\") for rec in records if not has_signal(rec)\n",
        "]\n",
        "no_signal_titles[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "normalize_whitespace and normalize_date ready.\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Define normalize_whitespace and normalize_date for export.\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "def normalize_whitespace(text: str | None) -> str:\n",
        "    \"\"\"Collapse whitespace to single spaces and strip; used for export fields.\"\"\"\n",
        "    return re.sub(r\"\\s+\", \" \", text or \"\").strip()\n",
        "\n",
        "\n",
        "def normalize_date(value: str | None) -> str:\n",
        "    \"\"\"Best-effort normalization to YYYY-MM-DD; handles '2026 Jan 7', '2025 Dec', '2025'. Falls back to original if we can't parse.\"\"\"\n",
        "    value = (value or \"\").strip()\n",
        "    if not value:\n",
        "        return \"\"\n",
        "    # Examples observed: \"2026 Jan 7\", \"2025 Dec\", \"2025\"\n",
        "    try:\n",
        "        return datetime.strptime(value, \"%Y %b %d\").strftime(\"%Y-%m-%d\")\n",
        "    except ValueError:\n",
        "        pass\n",
        "    try:\n",
        "        return datetime.strptime(value, \"%Y %b\").strftime(\"%Y-%m-01\")\n",
        "    except ValueError:\n",
        "        pass\n",
        "    if re.fullmatch(r\"\\d{4}\", value):\n",
        "        return f\"{value}-01-01\"\n",
        "    return value\n",
        "\n",
        "print(\"normalize_whitespace and normalize_date ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "5",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/px/lf9z17f12td3nckm6nvv2w380000gn/T/ipykernel_52055/3977385801.py:5: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  RUN_DATE: str = datetime.utcnow().strftime(\"%Y%m%d\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('data/pubmed_records_20260130.jsonl', 500)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cell 5: Build record_docs (id, text, metadata), write JSONL to data/pubmed_records_YYYYMMDD.jsonl.\n",
        "from typing import Any\n",
        "\n",
        "OUTPUT_DIR: str = \"data\"\n",
        "RUN_DATE: str = datetime.utcnow().strftime(\"%Y%m%d\")\n",
        "OUTPUT_PATH: str = os.path.join(OUTPUT_DIR, f\"pubmed_records_{RUN_DATE}.jsonl\")\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "record_docs: list[dict[str, Any]] = []\n",
        "for rec in records:\n",
        "    title = normalize_whitespace(rec.get(\"title\", \"\"))\n",
        "    abstract = normalize_whitespace(rec.get(\"abstract\", \"\"))\n",
        "    full_text = \"\\n\".join([t for t in [title, abstract] if t])\n",
        "    record_docs.append(\n",
        "        {\n",
        "            \"id\": rec.get(\"pmid\"),\n",
        "            \"text\": full_text,\n",
        "            \"metadata\": {\n",
        "                \"pmid\": rec.get(\"pmid\"),\n",
        "                \"title\": title,\n",
        "                \"journal\": rec.get(\"journal\"),\n",
        "                \"authors\": rec.get(\"authors\"),\n",
        "                \"date\": normalize_date(rec.get(\"date\")),\n",
        "                \"source\": \"pubmed_fetch\",\n",
        "            },\n",
        "        }\n",
        "    )\n",
        "\n",
        "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as handle:\n",
        "    for doc in record_docs:\n",
        "        handle.write(json.dumps(doc, ensure_ascii=True) + \"\\n\")\n",
        "\n",
        "OUTPUT_PATH, len(record_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archived 2 JSONL files to s3://pubmed-rag-data/processed/archive/\n",
            "Uploaded to s3://pubmed-rag-data/processed/pubmed_records_20260130.jsonl\n"
          ]
        }
      ],
      "source": [
        "# Cell 6: Optional — upload JSONL to S3 processed/ prefix (skipped if S3_BUCKET not set).\n",
        "load_env(reload=True)\n",
        "S3_BUCKET: str = os.getenv(\"S3_BUCKET\", \"\")\n",
        "S3_PREFIX: str = os.getenv(\"S3_PREFIX\", \"processed/\")\n",
        "ARCHIVE_PREFIX: str = os.getenv(\"S3_ARCHIVE_PREFIX\", \"processed/archive/\")\n",
        "\n",
        "if not S3_BUCKET:\n",
        "    print(\"S3_BUCKET not set; skipping upload. Processed output is in OUTPUT_PATH.\")\n",
        "else:\n",
        "    import boto3\n",
        "\n",
        "    s3 = boto3.client(\"s3\")\n",
        "    source_prefix = S3_PREFIX if S3_PREFIX.endswith(\"/\") else f\"{S3_PREFIX}/\"\n",
        "    archive_prefix = (\n",
        "        ARCHIVE_PREFIX if ARCHIVE_PREFIX.endswith(\"/\") else f\"{ARCHIVE_PREFIX}/\"\n",
        "    )\n",
        "\n",
        "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
        "    moved = 0\n",
        "    for page in paginator.paginate(Bucket=S3_BUCKET, Prefix=source_prefix):\n",
        "        for obj in page.get(\"Contents\", []):\n",
        "            key = obj.get(\"Key\", \"\")\n",
        "            if not key.endswith(\".jsonl\"):\n",
        "                continue\n",
        "            dest_key = key.replace(source_prefix, archive_prefix, 1)\n",
        "            s3.copy_object(\n",
        "                Bucket=S3_BUCKET,\n",
        "                CopySource={\"Bucket\": S3_BUCKET, \"Key\": key},\n",
        "                Key=dest_key,\n",
        "            )\n",
        "            s3.delete_object(Bucket=S3_BUCKET, Key=key)\n",
        "            moved += 1\n",
        "\n",
        "    print(f\"Archived {moved} JSONL files to s3://{S3_BUCKET}/{archive_prefix}\")\n",
        "\n",
        "    upload_key = f\"{source_prefix}{os.path.basename(OUTPUT_PATH)}\"\n",
        "    s3.upload_file(OUTPUT_PATH, S3_BUCKET, upload_key)\n",
        "    print(f\"Uploaded to s3://{S3_BUCKET}/{upload_key}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Started ingestion job: FHQWRGQRLK\n"
          ]
        }
      ],
      "source": [
        "# Cell 7: Start Bedrock KB ingestion job (requires BEDROCK_KB_ID, BEDROCK_KB_DATA_SOURCE_ID).\n",
        "import json\n",
        "import os\n",
        "import subprocess\n",
        "from typing import Any\n",
        "\n",
        "load_env(reload=True)\n",
        "\n",
        "KB_ID: str = os.getenv(\"BEDROCK_KB_ID\", \"\")\n",
        "DATA_SOURCE_ID: str = os.getenv(\"BEDROCK_KB_DATA_SOURCE_ID\", \"\")\n",
        "\n",
        "if not KB_ID or not DATA_SOURCE_ID:\n",
        "    print(\n",
        "        \"BEDROCK_KB_ID or BEDROCK_KB_DATA_SOURCE_ID not set; skipping ingestion. \"\n",
        "        \"Set both in .env or your shell to start a KB sync job.\"\n",
        "    )\n",
        "else:\n",
        "    result: subprocess.CompletedProcess[str] = subprocess.run(\n",
        "        [\n",
        "            \"aws\",\n",
        "            \"bedrock-agent\",\n",
        "            \"start-ingestion-job\",\n",
        "            \"--knowledge-base-id\",\n",
        "            KB_ID,\n",
        "            \"--data-source-id\",\n",
        "            DATA_SOURCE_ID,\n",
        "        ],\n",
        "        check=True,\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "    )\n",
        "\n",
        "    payload: dict[str, Any] = json.loads(result.stdout)\n",
        "    job_id: str = payload[\"ingestionJob\"][\"ingestionJobId\"]\n",
        "    print(f\"Started ingestion job: {job_id}\")\n",
        "    {\"knowledgeBaseId\": KB_ID, \"dataSourceId\": DATA_SOURCE_ID, \"ingestionJobId\": job_id}"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pubmed-rag-system",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
