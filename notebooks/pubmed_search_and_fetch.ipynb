{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0",
      "metadata": {},
      "source": [
        "# Mamoru PubMed Search + Fetch\n",
        "\n",
        "## Notebook outline\n",
        "\n",
        "| Cell | Purpose |\n",
        "|------|---------|\n",
        "| 1 | Load env (NCBI_EMAIL, NCBI_API_KEY), set MeSH + tiab query, run ESearch (get WebEnv/QueryKey), print summary |\n",
        "| 2 | Fetch paper abstracts in batches, format, write .txt to data/pubmed_fetch/. |\n",
        "| 3 | Cleanup by deleting the local `data/pubmed_fetch/` folder |\n",
        "| 4 | Upload fetched `.txt` files to S3 `raw/` prefix (requires S3_BUCKET) |\n",
        "\n",
        "---\n",
        "\n",
        "**Prerequisites.** None (run this notebook first to populate `data/pubmed_fetch/`).\n",
        "\n",
        "**Env.** `NCBI_EMAIL` (required), `NCBI_API_KEY` (optional), `S3_BUCKET` (cell 4). Set in `.env` or your shell. To create an API key: https://account.ncbi.nlm.nih.gov/settings/\n",
        "\n",
        "**Reference.** https://biopython.org/docs/latest/Tutorial/chapter_entrez.html#esearch-searching-the-entrez-databases\n",
        "\n",
        "**Note about MEDLINE format.** PubMed returns records in **MEDLINE** format: a standard tagged text format (e.g. `PMID-`, `TI-`, `AB-`) that we parse into structured metadata and abstract text. In this notebook we pull that data out and write one `.txt` per record. Downstream (see the processing notebook), we process the paper metadata and **chunk and embed the abstract text** into vectors so it can be indexed in the RAG knowledge base for semantic search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'count': 15669,\n",
              " 'retmax': 500,\n",
              " 'returned_ids': 500,\n",
              " 'query_translation': '\"Dementia\"[MeSH Terms] AND (\"decision support systems, clinical\"[MeSH Terms] OR \"Caregivers\"[MeSH Terms] OR \"caregiver*\"[Title/Abstract] OR \"decision support\"[Title/Abstract])',\n",
              " 'webenv': 'MCID_697c0fd33ad238690c0b587b',\n",
              " 'query_key': '1'}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cell 1: Load env, define query, run ESearch, print summary.\n",
        "import os\n",
        "import time\n",
        "from typing import Any, Iterator\n",
        "\n",
        "from Bio import Entrez, Medline\n",
        "\n",
        "DEFAULT_QUERY: str = (\n",
        "    '(\"Dementia\"[Mesh] OR \"Mild Cognitive Impairment\"[Mesh]) '\n",
        "    'AND (\"Decision Support Systems, Clinical\"[Mesh] OR \"Caregivers\"[Mesh] '\n",
        "    'OR caregiver*[tiab] OR \"decision support\"[tiab])'\n",
        ")\n",
        "DEFAULT_RETMAX: int = 500\n",
        "DEFAULT_FETCH_DIR: str = \"data/pubmed_fetch\"\n",
        "RAW_PREFIX: str = \"raw\"\n",
        "\n",
        "\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "\n",
        "    load_dotenv()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "\n",
        "def load_env(reload: bool = False) -> tuple[str, str, str]:\n",
        "    \"\"\"Load env vars and validate required settings.\"\"\"\n",
        "    if reload:\n",
        "        try:\n",
        "            from dotenv import load_dotenv\n",
        "\n",
        "            load_dotenv(override=True)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    email = os.getenv(\"NCBI_EMAIL\", \"\")\n",
        "    api_key = os.getenv(\"NCBI_API_KEY\", \"\")\n",
        "    s3_bucket = os.getenv(\"S3_BUCKET\", \"\")\n",
        "    if not email:\n",
        "        raise ValueError(\"NCBI_EMAIL must be set in your environment or .env\")\n",
        "    return email, api_key, s3_bucket\n",
        "\n",
        "\n",
        "def configure_entrez(email: str, api_key: str) -> None:\n",
        "    \"\"\"Configure Biopython Entrez settings.\"\"\"\n",
        "    Entrez.email = email\n",
        "    if api_key:\n",
        "        Entrez.api_key = api_key\n",
        "\n",
        "\n",
        "def run_esearch(query: str, retmax: int) -> dict:\n",
        "    \"\"\"Run a PubMed ESearch and return the result; usehistory='y' enables batched EFetch via WebEnv/QueryKey.\"\"\"\n",
        "    try:\n",
        "        stream = Entrez.esearch(db=\"pubmed\", term=query, retmax=retmax, usehistory=\"y\")\n",
        "        record = Entrez.read(stream)\n",
        "        stream.close()\n",
        "        return record\n",
        "    except Exception as exc:\n",
        "        raise RuntimeError(f\"PubMed search failed: {exc}\") from exc\n",
        "\n",
        "\n",
        "def _format_record(rec: dict[str, Any]) -> str:\n",
        "    \"\"\"Format a parsed MEDLINE record into a single text block for our .txt files (PMID, title, authors, journal, date, abstract).\"\"\"\n",
        "    parts = []\n",
        "    if rec.get(\"PMID\"):\n",
        "        parts.append(f\"PMID: {rec['PMID']}\")\n",
        "    if rec.get(\"TI\"):\n",
        "        parts.append(f\"Title: {rec['TI']}\")\n",
        "    if rec.get(\"AU\"):\n",
        "        parts.append(f\"Authors: {', '.join(rec['AU'])}\")\n",
        "    if rec.get(\"JT\"):\n",
        "        parts.append(f\"Journal: {rec['JT']}\")\n",
        "    if rec.get(\"DP\"):\n",
        "        parts.append(f\"Date: {rec['DP']}\")\n",
        "    if rec.get(\"AB\"):\n",
        "        parts.append(f\"Abstract:\\n{rec['AB']}\")\n",
        "    return \"\\n\".join(parts).strip()\n",
        "\n",
        "\n",
        "def iter_efetch_batches(\n",
        "    webenv: str | None,\n",
        "    query_key: str | None,\n",
        "    target_count: int,\n",
        "    batch_size: int = 100,\n",
        "    request_delay_sec: float = 0.34,\n",
        ") -> Iterator[Any]:\n",
        "    \"\"\"Yield EFetch response streams for each batch; caller parses and closes each stream. Yields: open file-like from Entrez.efetch (MEDLINE).\"\"\"\n",
        "    if not (webenv and query_key):\n",
        "        return\n",
        "    for start in range(0, target_count, batch_size):\n",
        "        stream = Entrez.efetch(\n",
        "            db=\"pubmed\",\n",
        "            rettype=\"medline\",\n",
        "            retmode=\"text\",\n",
        "            retstart=start,\n",
        "            retmax=min(batch_size, target_count - start),\n",
        "            webenv=webenv,\n",
        "            query_key=query_key,\n",
        "        )\n",
        "        try:\n",
        "            yield stream\n",
        "        finally:\n",
        "            stream.close()\n",
        "        if start + batch_size < target_count:\n",
        "            time.sleep(request_delay_sec)\n",
        "\n",
        "\n",
        "def fetch_and_write_medline_records(\n",
        "    record: dict[str, Any],\n",
        "    retmax: int,\n",
        "    fetch_dir: str,\n",
        "    batch_size: int = 100,\n",
        "    request_delay_sec: float = 0.34,\n",
        ") -> int:\n",
        "    \"\"\"Fetch MEDLINE in batches from an ESearch result, format each record, write one .txt per PMID; returns count written.\"\"\"\n",
        "    webenv = record.get(\"WebEnv\")\n",
        "    query_key = record.get(\"QueryKey\")\n",
        "    total_count = int(record.get(\"Count\", 0))\n",
        "    target_count = min(retmax, total_count)\n",
        "\n",
        "    written = 0\n",
        "    for stream in iter_efetch_batches(\n",
        "        webenv, query_key, target_count, batch_size, request_delay_sec\n",
        "    ):\n",
        "        for rec in Medline.parse(stream):\n",
        "            pmid = rec.get(\"PMID\")\n",
        "            if not pmid:\n",
        "                continue\n",
        "            text = _format_record(rec)\n",
        "            if not text:\n",
        "                continue\n",
        "            out_path = os.path.join(fetch_dir, f\"{pmid}.txt\")\n",
        "            with open(out_path, \"w\", encoding=\"utf-8\") as handle:\n",
        "                handle.write(text)\n",
        "            written += 1\n",
        "    return written\n",
        "\n",
        "\n",
        "def get_request_delay(api_key: str) -> float:\n",
        "    \"\"\"Shorter delay when API key is set (NCBI allows higher rate).\"\"\"\n",
        "    return 0.10 if api_key else 0.34\n",
        "\n",
        "\n",
        "def delete_fetch_dir(fetch_dir: str, confirm: bool = False) -> str:\n",
        "    \"\"\"Delete the fetch directory when confirm=True.\"\"\"\n",
        "    if not confirm:\n",
        "        return \"Cleanup skipped. Set confirm=True to delete local data.\"\n",
        "    import shutil\n",
        "\n",
        "    if os.path.isdir(fetch_dir):\n",
        "        shutil.rmtree(fetch_dir)\n",
        "        return \"Deleted fetched data folder.\"\n",
        "    return \"Nothing to delete.\"\n",
        "\n",
        "\n",
        "def upload_fetch_dir(fetch_dir: str, bucket: str, raw_prefix: str = RAW_PREFIX) -> int:\n",
        "    \"\"\"Upload .txt files from fetch_dir to S3 raw/ prefix; returns count uploaded.\"\"\"\n",
        "    import boto3\n",
        "\n",
        "    s3 = boto3.client(\"s3\")\n",
        "    uploaded = 0\n",
        "    for filename in os.listdir(fetch_dir):\n",
        "        if not filename.endswith(\".txt\"):\n",
        "            continue\n",
        "        local_path = os.path.join(fetch_dir, filename)\n",
        "        key = f\"{raw_prefix}/{filename}\"\n",
        "        s3.upload_file(local_path, bucket, key)\n",
        "        uploaded += 1\n",
        "    return uploaded\n",
        "\n",
        "\n",
        "EMAIL, API_KEY, _ = load_env()\n",
        "configure_entrez(EMAIL, API_KEY)\n",
        "\n",
        "# MeSH + tiab query for dementia caregiving; usehistory enables batched EFetch.\n",
        "QUERY = DEFAULT_QUERY\n",
        "RETMAX = DEFAULT_RETMAX\n",
        "FETCH_DIR = DEFAULT_FETCH_DIR\n",
        "\n",
        "record = run_esearch(QUERY, RETMAX)\n",
        "summary = {\n",
        "    \"count\": int(record.get(\"Count\", 0)),\n",
        "    \"retmax\": int(record.get(\"RetMax\", 0)),\n",
        "    \"returned_ids\": len(record.get(\"IdList\", [])),\n",
        "    \"query_translation\": record.get(\"QueryTranslation\"),\n",
        "    \"webenv\": record.get(\"WebEnv\"),\n",
        "    \"query_key\": record.get(\"QueryKey\"),\n",
        "}\n",
        "summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Wrote 500 records to data/pubmed_fetch.'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cell 2: Fetch paper abstracts in batches, format, write .txt to data/pubmed_fetch/.\n",
        "os.makedirs(FETCH_DIR, exist_ok=True)\n",
        "\n",
        "request_delay = get_request_delay(API_KEY)\n",
        "written = fetch_and_write_medline_records(\n",
        "    record, RETMAX, FETCH_DIR, batch_size=100, request_delay_sec=request_delay\n",
        ")\n",
        "f\"Wrote {written} records to {FETCH_DIR}.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Cleanup skipped. Set confirm=True to delete local data.'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cell 3: Cleanup â€” delete local data/pubmed_fetch/ folder.\n",
        "confirm_delete = False\n",
        "\n",
        "delete_fetch_dir(FETCH_DIR, confirm=confirm_delete)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Uploaded 500 files to s3://pubmed-rag-data/raw/'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cell 4: Upload fetched .txt files to S3 raw/ prefix if S3_BUCKET is set.\n",
        "_, _, S3_BUCKET = load_env(reload=True)\n",
        "\n",
        "if not S3_BUCKET:\n",
        "    raise ValueError(\"S3_BUCKET must be set in your environment or .env\")\n",
        "\n",
        "uploaded = upload_fetch_dir(FETCH_DIR, S3_BUCKET, raw_prefix=RAW_PREFIX)\n",
        "\n",
        "f\"Uploaded {uploaded} files to s3://{S3_BUCKET}/{RAW_PREFIX}/\""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pubmed-rag-system",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
