{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Mamoru PubMed Search + Fetch\n",
    "\n",
    "## Notebook outline\n",
    "\n",
    "| Cell | Purpose |\n",
    "|------|---------|\n",
    "| 1 | Load env (NCBI_EMAIL, NCBI_API_KEY), set MeSH + tiab query, run ESearch (get WebEnv/QueryKey), print summary |\n",
    "| 2 | Fetch paper abstracts in batches, format, write .txt to data/pubmed_fetch/. |\n",
    "| 3 | Cleanup by deleting the local `data/pubmed_fetch/` folder |\n",
    "| 4 | Upload fetched `.txt` files to S3 `raw/` prefix (requires S3_BUCKET) |\n",
    "\n",
    "---\n",
    "\n",
    "Set your email and optional NCBI API key as environment variables before running.\n",
    "\n",
    "To create an API key, sign in at https://account.ncbi.nlm.nih.gov/settings/ and generate an access key.\n",
    "\n",
    "Example env var setup:\n",
    "- `export NCBI_EMAIL=\"you@example.com\"`\n",
    "- `export NCBI_API_KEY=\"YOUR_KEY\"`\n",
    "- `export S3_BUCKET=\"your-bucket\"`\n",
    "\n",
    "Optional: create a local `.env` (gitignored) which will be loaded with `python-dotenv`.\n",
    "\n",
    "Reference: https://biopython.org/docs/latest/Tutorial/chapter_entrez.html#esearch-searching-the-entrez-databases\n",
    "\n",
    "**Note about MEDLINE formtat.** PubMed returns records in **MEDLINE** format: a standard tagged text format (e.g. `PMID-`, `TI-`, `AB-`) that we parse into structured metadata and abstract text. In this notebook we pull that data out and write one `.txt` per record. Downstream (see the processing notebook), we process the paper metadata and **chunk and embed the abstract text** into vectors so it can be indexed in the RAG knowledge base for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Load env, define query, run ESearch, print summary.\n",
    "import os\n",
    "from Bio import Entrez\n",
    "\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "EMAIL = os.getenv(\"NCBI_EMAIL\")\n",
    "API_KEY = os.getenv(\"NCBI_API_KEY\", \"\")\n",
    "\n",
    "if not EMAIL:\n",
    "    raise ValueError(\"NCBI_EMAIL must be set in your environment or .env\")\n",
    "\n",
    "Entrez.email = EMAIL\n",
    "if API_KEY:\n",
    "    Entrez.api_key = API_KEY\n",
    "\n",
    "\n",
    "def run_esearch(query: str, retmax: int) -> dict:\n",
    "    \"\"\"Run a PubMed search and return the ESearch result.\n",
    "\n",
    "    We use usehistory='y' so we can fetch results in batches later with EFetch\n",
    "    (WebEnv and QueryKey). Relies on Entrez.email and Entrez.api_key from this notebook.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        stream = Entrez.esearch(db=\"pubmed\", term=query, retmax=retmax, usehistory=\"y\")\n",
    "        record = Entrez.read(stream)\n",
    "        stream.close()\n",
    "        return record\n",
    "    except Exception as exc:\n",
    "        raise RuntimeError(f\"PubMed search failed: {exc}\") from exc\n",
    "\n",
    "\n",
    "# MeSH + tiab query for dementia caregiving; usehistory enables batched EFetch.\n",
    "QUERY = (\n",
    "    '(\"Dementia\"[Mesh] OR \"Mild Cognitive Impairment\"[Mesh]) '\n",
    "    'AND (\"Decision Support Systems, Clinical\"[Mesh] OR \"Caregivers\"[Mesh] '\n",
    "    'OR caregiver*[tiab] OR \"decision support\"[tiab])'\n",
    ")\n",
    "RETMAX = 500\n",
    "\n",
    "record = run_esearch(QUERY, RETMAX)\n",
    "summary = {\n",
    "    \"count\": int(record.get(\"Count\", 0)),\n",
    "    \"retmax\": int(record.get(\"RetMax\", 0)),\n",
    "    \"returned_ids\": len(record.get(\"IdList\", [])),\n",
    "    \"query_translation\": record.get(\"QueryTranslation\"),\n",
    "    \"webenv\": record.get(\"WebEnv\"),\n",
    "    \"query_key\": record.get(\"QueryKey\"),\n",
    "}\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Fetch paper abstracts in batches, format, write .txt to data/pubmed_fetch/.\n",
    "import os\n",
    "import time\n",
    "from typing import Any, Iterator\n",
    "\n",
    "from Bio import Entrez, Medline\n",
    "\n",
    "FETCH_DIR = \"data/pubmed_fetch\"\n",
    "os.makedirs(FETCH_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def _format_record(rec: dict[str, Any]) -> str:\n",
    "    \"\"\"Format a parsed MEDLINE record into a single text block for our .txt files.\n",
    "\n",
    "    Pulls out PMID, title, authors, journal, date, and abstract when they're there.\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    if rec.get(\"PMID\"):\n",
    "        parts.append(f\"PMID: {rec['PMID']}\")\n",
    "    if rec.get(\"TI\"):\n",
    "        parts.append(f\"Title: {rec['TI']}\")\n",
    "    if rec.get(\"AU\"):\n",
    "        parts.append(f\"Authors: {', '.join(rec['AU'])}\")\n",
    "    if rec.get(\"JT\"):\n",
    "        parts.append(f\"Journal: {rec['JT']}\")\n",
    "    if rec.get(\"DP\"):\n",
    "        parts.append(f\"Date: {rec['DP']}\")\n",
    "    if rec.get(\"AB\"):\n",
    "        parts.append(f\"Abstract:\\n{rec['AB']}\")\n",
    "    return \"\\n\".join(parts).strip()\n",
    "\n",
    "\n",
    "def iter_efetch_batches(\n",
    "    webenv: str | None,\n",
    "    query_key: str | None,\n",
    "    target_count: int,\n",
    "    batch_size: int = 100,\n",
    "    request_delay_sec: float = 0.34,\n",
    ") -> Iterator[Any]:\n",
    "    \"\"\"Yield EFetch response streams for each batch (caller parses and closes each stream).\n",
    "\n",
    "    Args:\n",
    "        webenv: ESearch WebEnv (required with query_key).\n",
    "        query_key: ESearch QueryKey (required with webenv).\n",
    "        target_count: Total records to fetch across batches.\n",
    "        batch_size: Records per EFetch request.\n",
    "        request_delay_sec: Sleep between batches for rate limiting.\n",
    "\n",
    "    Yields:\n",
    "        Open file-like stream from Entrez.efetch (rettype='medline'); caller must consume\n",
    "        then stream is closed before the next yield.\n",
    "    \"\"\"\n",
    "    if not (webenv and query_key):\n",
    "        return\n",
    "    for start in range(0, target_count, batch_size):\n",
    "        stream = Entrez.efetch(\n",
    "            db=\"pubmed\",\n",
    "            rettype=\"medline\",\n",
    "            retmode=\"text\",\n",
    "            retstart=start,\n",
    "            retmax=min(batch_size, target_count - start),\n",
    "            webenv=webenv,\n",
    "            query_key=query_key,\n",
    "        )\n",
    "        try:\n",
    "            yield stream\n",
    "        finally:\n",
    "            stream.close()\n",
    "        if start + batch_size < target_count:\n",
    "            time.sleep(request_delay_sec)\n",
    "\n",
    "\n",
    "def fetch_and_write_medline_records(\n",
    "    record: dict[str, Any],\n",
    "    retmax: int,\n",
    "    fetch_dir: str,\n",
    "    batch_size: int = 100,\n",
    "    request_delay_sec: float = 0.34,\n",
    ") -> int:\n",
    "    \"\"\"Fetch MEDLINE in batches from PubMed, format each record, and write one .txt per PMID.\n",
    "\n",
    "    Expects an ESearch record (WebEnv, QueryKey, Count). We cap at retmax records,\n",
    "    sleep between batches to respect rate limits, and return how many files we wrote.\n",
    "    \"\"\"\n",
    "    webenv = record.get(\"WebEnv\")\n",
    "    query_key = record.get(\"QueryKey\")\n",
    "    total_count = int(record.get(\"Count\", 0))\n",
    "    target_count = min(retmax, total_count)\n",
    "\n",
    "    written = 0\n",
    "    for stream in iter_efetch_batches(\n",
    "        webenv, query_key, target_count, batch_size, request_delay_sec\n",
    "    ):\n",
    "        for rec in Medline.parse(stream):\n",
    "            pmid = rec.get(\"PMID\")\n",
    "            if not pmid:\n",
    "                continue\n",
    "            text = _format_record(rec)\n",
    "            if not text:\n",
    "                continue\n",
    "            out_path = os.path.join(fetch_dir, f\"{pmid}.txt\")\n",
    "            with open(out_path, \"w\", encoding=\"utf-8\") as handle:\n",
    "                handle.write(text)\n",
    "            written += 1\n",
    "    return written\n",
    "\n",
    "\n",
    "request_delay = 0.10\n",
    "written = fetch_and_write_medline_records(\n",
    "    record, RETMAX, FETCH_DIR, batch_size=100, request_delay_sec=request_delay\n",
    ")\n",
    "f\"Wrote {written} records to {FETCH_DIR}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 3: Cleanup: delete local data/pubmed_fetch/ folder.  Commented out to avoid accidentally running.\n",
    "# import shutil\n",
    "\n",
    "# if os.path.isdir(FETCH_DIR):\n",
    "#     shutil.rmtree(FETCH_DIR)\n",
    "#     \"Deleted fetched data folder.\"\n",
    "# else:\n",
    "#     \"Nothing to delete.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Upload fetched .txt files to S3 raw/ prefix if S3_BUCKET is set.\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "S3_BUCKET = os.getenv(\"S3_BUCKET\")\n",
    "RAW_PREFIX = \"raw\"\n",
    "\n",
    "if not S3_BUCKET:\n",
    "    raise ValueError(\"S3_BUCKET must be set in your environment or .env\")\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "uploaded = 0\n",
    "for filename in os.listdir(FETCH_DIR):\n",
    "    if not filename.endswith(\".txt\"):\n",
    "        continue\n",
    "    local_path = os.path.join(FETCH_DIR, filename)\n",
    "    key = f\"{RAW_PREFIX}/{filename}\"\n",
    "    s3.upload_file(local_path, S3_BUCKET, key)\n",
    "    uploaded += 1\n",
    "\n",
    "f\"Uploaded {uploaded} files to s3://{S3_BUCKET}/{RAW_PREFIX}/\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pubmed-rag-system",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
